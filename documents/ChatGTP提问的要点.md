# ChatGPT提问的要点
这个视频主要讲解了如何优化提问用到的提示词,然后应用到我们的程序中.

原视频并且带练习功能的网址: https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/2/guidelines  
带中文翻译的视频的网址: https://cloudreve.jimmylab.cn/video?p=%2Fchatgpt%202.mp4&id=w6iG

Markdown语法:https://markdown.com.cn/basic-syntax/

### 创意:
1. 为儿童优化的十万个为什么
2. **将故事脚本和图片生成结合在一起,方便生成绘本的工具**
3. 帮助儿童学习中文和英文的阅读理解,语法等知识的工具
4. **从第四个视频的启发,可以做一些分析工具,比如用爬虫抓取关于某个股票相关的新闻,然后用ChatGPT分析一下,为未来走势做判断**
5. 翻译助手,翻译后用微软语音朗读出来
6. 聊天机器人,根据聊天内容的情绪,聊天机器人提供不同的头像表情或头像动画.
7. 构建一个反诈骗话术的工具

视频提到了30个完美的提示词,可以找来看一下

## 第二节
### 在给出问题的时候要明确和具体
可以像给刚毕业的大学生布置任务一样, 可以列出具体,明确的步骤.更长更清晰的问题和上下文,可以让模型更好的理解并输出想要的结果.比如写道:"你的任务是xxxxx".
1. **可以使用分隔符,告诉模型,哪些是对它的要求,哪些是材料**  
   视频中的prompt,以f"""开头以"""结尾 (f"""sdfsdfs""").这是使用了python的f-string功能.它起到的功能和TS中的``类似,可以在字符串中,使用{xxx}的方式插入变量/表达式的值.
   f-string功能不是必须用三个双引号.用单引号,双引号,三引号都行.对于只有一行的f-string,可以使用单引号或双引号来定义字符串.对于多行的f-string,可以使用三引号(三个单引号或三个双引号)来定义字符串.

   使用什么作为分隔符都可以,只要能清晰分隔就行
   
2. **要求模型给出结构化输出,比如以html或json输出**  
   这中json格式输出的数据,可以被程序代码直接使用

3. **要求模型检查是否满足条件,如果满足假定的条件就继续输出,不满足就停止**  
   视频中的例子是要求模型整理出制茶的步骤.如果给出的文本里面包含步骤,就按照步骤输出,否则就提示没有步骤. 我们还可以考虑潜在的边缘的情况,防止模型进行错误的输出(比如输出错误内容给用户,不如给出合理的理由,告诉用户不能输出想要的结果)
   
4. **可以在提问之前先简单训练一下**  
   在正式提问之前,可以先举个例子,告诉模型,回答我的问题,正确的回答是什么样的.然后再继续问出问题.这样模型相当于被训练了,就可以用正确的方式回答

### 要求模型在问题上花更多的时间思考,这样可以获得更多的算力
1. **可以告诉模型完成任务所需的步骤,每一步要做什么,把步骤列出来**  
   比如列出Step1:做什么什么, Step2:做什么什么,以此类推. 然后再列出输出答案的格式.
   
2. **可以让模型自己演算一遍,再判断问题的答案**
   比如视频中的例子.需要模型判断学生的解决方案是否正确,那么可以让模型一步一步的自己解算一遍.然后再与学生的方案对比,从而得出是否正确的结论.
   如果没有这个模型自己的方案和学生方案的对比,模型给出的答案会是错的

### 模型的局限性
1. **关于一些比较晦涩的问题,模型可能会给出逼真但是完全错误的回答,被称为"幻觉"**  
   解决方法是让模型从问题的文本中找出任何相关的引用,并且用这些引用来回答问题,并且可以追溯答案到原文档.这样可以减少"幻觉".
   
## 迭代提示开发
通过向模型提问,得到结果,然后分析结果中有哪些不足,然后改进提问的问题,比如增加更多的明确的限制条件,再继续询问.不断改进提问的prompt,直到得到满意的结果.最后满意的prompt就是程序可以使用的prompt.这个类似代码调试从而得到最终的代码一样.

## LLM(Large Language Model)模型最擅长给文字做总结,摘要,提取关键信息,转换(翻译,语法纠正,HTML转JSON),扩展等等自然语言的处理
1. 模型可以从产品评论中推测出积极或消极的情绪
2. 模型可以从产品评论中提取出特定信息,比如购买的东西,产品的制造公司
3. 模型可以将一句短的话扩展成更长的一段文本.可以根据文本的内容和情感,产生回复信息.

## 第八节
提到了temperature,用来控制模型响应的多样性,它可以被视作是模型的探索程度或随机性.
如果temperature是0,那么模型只会返回可能性最高的预测.如果temperature达到0.7,那么只有5%可能性的答案也是可能被返回的.
在构建需要可靠的,可预测的回复的应用程序时,temperature要设置成0.
如果需要构建更有创意的,获得更广泛的不同输出的应用时,可以选择更高的temperature.
也就是说,如果temperature设置成0,那么可以预期每次得到的回复都相同.如果设置得更高,那么每次的回复都不相同.

## 第八节
在创建聊天机器人的时候,在消息列表里面,第一句由system role发出的"You are an assistant...",有助于设置机器人(模型)的行为和人设.并作为高层指令用于对话,相当于在机器人旁边耳语,引导它的回应.

模型接收的每一次请求,都是一次独立的对话.模型不会知道以前的事情.为了能够让模型"记住",需要将上下文记录并且让模型知道.那么可以使用message列表,将一组对话和回复放到里面,然后把列表发给模型,这样模型就会根据列表中的内容,知道上下文,从而可以继续对话.例子中也是这样做的,将模型和用户的信息,动态存放在列表中,然后再一块儿发给模型.
我猜测Bing就是这样做的,所以限制一次会话中对话的次数.

在例子中,system role给机器人设置了详细的行为指导. 比如you first greet, wait to collect, if...
